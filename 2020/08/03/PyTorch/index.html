<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"fanchl.github.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Torch &amp; NumpyTorch 自称为神经网络界的 Numpy, 能将 torch 产生的 tensor 放在 GPU 中加速运算。">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch 基础使用">
<meta property="og:url" content="https://fanchl.github.com/2020/08/03/PyTorch/index.html">
<meta property="og:site_name" content="fanchl&#39;s blog">
<meta property="og:description" content="Torch &amp; NumpyTorch 自称为神经网络界的 Numpy, 能将 torch 产生的 tensor 放在 GPU 中加速运算。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-08-02T16:00:00.000Z">
<meta property="article:modified_time" content="2021-05-16T03:04:38.508Z">
<meta property="article:author" content="fanchl">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://fanchl.github.com/2020/08/03/PyTorch/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>PyTorch 基础使用 | fanchl's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">fanchl's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">模式识别研究生</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://fanchl.github.com/2020/08/03/PyTorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="fanchl">
      <meta itemprop="description" content="日拱一卒，功不唐捐。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="fanchl's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PyTorch 基础使用
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-08-03 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-03T00:00:00+08:00">2020-08-03</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Torch-amp-Numpy"><a href="#Torch-amp-Numpy" class="headerlink" title="Torch &amp; Numpy"></a>Torch &amp; Numpy</h2><p>Torch 自称为神经网络界的 Numpy, 能将 torch 产生的 tensor 放在 GPU 中加速运算。</p>
<span id="more"></span>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line">tensor2array = torch_data.numpy()</span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nnumpy array:&#x27;</span>, np_data,          <span class="comment"># [[0 1 2], [3 4 5]]</span></span><br><span class="line">    <span class="string">&#x27;\ntorch tensor:&#x27;</span>, torch_data,      <span class="comment">#  0  1  2 \n 3  4  5    [torch.LongTensor of size 2x3]</span></span><br><span class="line">    <span class="string">&#x27;\ntensor to array:&#x27;</span>, tensor2array, <span class="comment"># [[0 1 2], [3 4 5]]</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<h2 id="Torch-中的数学运算"><a href="#Torch-中的数学运算" class="headerlink" title="Torch 中的数学运算"></a>Torch 中的数学运算</h2><h3 id="普通计算"><a href="#普通计算" class="headerlink" title="普通计算"></a>普通计算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># abs 绝对值计算</span></span><br><span class="line">data = [-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">tensor = torch.FloatTensor(data)  <span class="comment"># 转换成32位浮点 tensor</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nabs&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;\nnumpy: &#x27;</span>, np.<span class="built_in">abs</span>(data),          <span class="comment"># [1 2 1 2]</span></span><br><span class="line">    <span class="string">&#x27;\ntorch: &#x27;</span>, torch.<span class="built_in">abs</span>(tensor)      <span class="comment"># [1 2 1 2]</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sin   三角函数 sin</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nsin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;\nnumpy: &#x27;</span>, np.sin(data),      <span class="comment"># [-0.84147098 -0.90929743  0.84147098  0.90929743]</span></span><br><span class="line">    <span class="string">&#x27;\ntorch: &#x27;</span>, torch.sin(tensor)  <span class="comment"># [-0.8415 -0.9093  0.8415  0.9093]</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mean  均值</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nmean&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;\nnumpy: &#x27;</span>, np.mean(data),         <span class="comment"># 0.0</span></span><br><span class="line">    <span class="string">&#x27;\ntorch: &#x27;</span>, torch.mean(tensor)     <span class="comment"># 0.0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># matrix multiplication 矩阵点乘</span></span><br><span class="line">data = [[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line">tensor = torch.FloatTensor(data)  <span class="comment"># 转换成32位浮点 tensor</span></span><br><span class="line"><span class="comment"># correct method</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nmatrix multiplication (matmul)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;\nnumpy: &#x27;</span>, np.matmul(data, data),     <span class="comment"># [[7, 10], [15, 22]]</span></span><br><span class="line">    <span class="string">&#x27;\ntorch: &#x27;</span>, torch.mm(tensor, tensor)   <span class="comment"># [[7, 10], [15, 22]]</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># !!!!  下面是错误的方法 !!!!</span></span><br><span class="line">data = np.array(data)</span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nmatrix multiplication (dot)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;\nnumpy: &#x27;</span>, data.dot(data),        <span class="comment"># [[7, 10], [15, 22]] 在numpy 中可行</span></span><br><span class="line">    <span class="string">&#x27;\ntorch: &#x27;</span>, tensor.dot(tensor)     <span class="comment"># torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="comment"># torch 中 Variable 模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 先生鸡蛋</span></span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="comment"># 把鸡蛋放到篮子里, requires_grad是参不参与误差反向传播, 要不要计算梯度</span></span><br><span class="line">variable = Variable(tensor, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(variable)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="Variable-计算-梯度"><a href="#Variable-计算-梯度" class="headerlink" title="Variable 计算, 梯度"></a>Variable 计算, 梯度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t_out = torch.mean(tensor*tensor)       <span class="comment"># x^2</span></span><br><span class="line">v_out = torch.mean(variable*variable)   <span class="comment"># x^2</span></span><br><span class="line"><span class="built_in">print</span>(t_out)</span><br><span class="line"><span class="built_in">print</span>(v_out)    <span class="comment"># 7.5</span></span><br></pre></td></tr></table></figure>

<p>Variable 计算时, 它在背景幕布后面一步步默默地搭建着一个庞大的系统, 叫做计算图, computational graph. 这个图是用来干嘛的? 原来是将所有的计算步骤 (节点) 都连接起来, 最后进行误差反向传递的时候, 一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来, 而 tensor 就没有这个能力啦.</p>
<h3 id="获取-Variable-里面的数据"><a href="#获取-Variable-里面的数据" class="headerlink" title="获取 Variable 里面的数据"></a>获取 Variable 里面的数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(variable)     <span class="comment">#  Variable 形式</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(variable.data)    <span class="comment"># tensor 形式</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(variable.data.numpy())    <span class="comment"># numpy 形式</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[[ 1.  2.]</span></span><br><span class="line"><span class="string"> [ 3.  4.]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><h3 id="建立数据集"><a href="#建立数据集" class="headerlink" title="建立数据集"></a>建立数据集</h3><p>创建一些假数据来模拟真实的情况. 比如一个一元二次函数: <code>y = a * x^2 + b</code>, 我们给 <code>y</code> 数据加上一点噪声来更加真实的展示它.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = torch.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)  <span class="comment"># x data (tensor), shape=(100, 1)</span></span><br><span class="line">y = x.<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.2</span>*torch.rand(x.size())                 <span class="comment"># noisy y data (tensor), shape=(100, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h3 id="建立神经网络"><a href="#建立神经网络" class="headerlink" title="建立神经网络"></a>建立神经网络</h3><p>建立一个神经网络我们可以直接运用 torch 中的体系. 先定义所有的层属性(<code>__init__()</code>), 然后再一层层搭建(<code>forward(x)</code>)层于层的关系链接. 建立关系的时候, 会用到激励函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment"># 激励函数都在这</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span>  <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()     <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        <span class="comment"># 定义每层用什么样的形式</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)   <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span>   <span class="comment"># 这同时也是 Module 中的 forward 功能</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.predict(x)             <span class="comment"># 输出值</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">1</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net)  <span class="comment"># net 的结构</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (1 -&gt; 10)</span></span><br><span class="line"><span class="string">  (predict): Linear (10 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># optimizer 是训练的工具</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.2</span>)  <span class="comment"># 传入 net 的所有参数, 学习率</span></span><br><span class="line">loss_func = torch.nn.MSELoss()      <span class="comment"># 预测值和真实值的误差计算公式 (均方差)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    prediction = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出预测值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(prediction, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br></pre></td></tr></table></figure>

<h3 id="可视化训练过程"><a href="#可视化训练过程" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># 画图</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着上面来</span></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># plot and show learning process</span></span><br><span class="line">        plt.cla()</span><br><span class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">        plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">&#x27;r-&#x27;</span>, lw=<span class="number">5</span>)</span><br><span class="line">        plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">&#x27;Loss=%.4f&#x27;</span> % loss.data.numpy(), fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;color&#x27;</span>:  <span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>



<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><h3 id="建立数据集-1"><a href="#建立数据集-1" class="headerlink" title="建立数据集"></a>建立数据集</h3><p>创建一些假数据来模拟真实的情况. 比如两个二次分布的数据, 不过他们的均值都不一样.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假数据</span></span><br><span class="line">n_data = torch.ones(<span class="number">100</span>, <span class="number">2</span>)         <span class="comment"># 数据的基本形态</span></span><br><span class="line">x0 = torch.normal(<span class="number">2</span>*n_data, <span class="number">1</span>)      <span class="comment"># 类型0 x data (tensor), shape=(100, 2)</span></span><br><span class="line">y0 = torch.zeros(<span class="number">100</span>)               <span class="comment"># 类型0 y data (tensor), shape=(100, )</span></span><br><span class="line">x1 = torch.normal(-<span class="number">2</span>*n_data, <span class="number">1</span>)     <span class="comment"># 类型1 x data (tensor), shape=(100, 1)</span></span><br><span class="line">y1 = torch.ones(<span class="number">100</span>)                <span class="comment"># 类型1 y data (tensor), shape=(100, )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意 x, y 数据的数据形式是一定要像下面一样 (torch.cat 是在合并数据)</span></span><br><span class="line">x = torch.cat((x0, x1), <span class="number">0</span>).<span class="built_in">type</span>(torch.FloatTensor)  <span class="comment"># FloatTensor = 32-bit floating</span></span><br><span class="line">y = torch.cat((y0, y1), ).<span class="built_in">type</span>(torch.LongTensor)    <span class="comment"># LongTensor = 64-bit integer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=y.data.numpy(), s=100, lw=0, cmap=&#x27;RdYlGn&#x27;)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h3 id="建立神经网络-1"><a href="#建立神经网络-1" class="headerlink" title="建立神经网络"></a>建立神经网络</h3><p>建立一个神经网络我们可以直接运用 torch 中的体系. 先定义所有的层属性(<code>__init__()</code>), 然后再一层层搭建(<code>forward(x)</code>)层于层的关系链接. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment"># 激励函数都在这</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span>     <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()     <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.out = torch.nn.Linear(n_hidden, n_output)       <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.out(x)                 <span class="comment"># 输出值, 但是这个不是预测值, 预测值还需要再另外计算</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">2</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">2</span>) <span class="comment"># 几个类别就几个 output</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net)  <span class="comment"># net 的结构</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (2 -&gt; 10)</span></span><br><span class="line"><span class="string">  (out): Linear (10 -&gt; 2)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="训练网络-1"><a href="#训练网络-1" class="headerlink" title="训练网络"></a>训练网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># optimizer 是训练的工具</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.02</span>)  <span class="comment"># 传入 net 的所有参数, 学习率</span></span><br><span class="line"><span class="comment"># 算误差的时候, 注意真实值!不是! one-hot 形式的, 而是1D Tensor, (batch,)</span></span><br><span class="line"><span class="comment"># 但是预测值是2D tensor (batch, n_classes)</span></span><br><span class="line">loss_func = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    out = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出分析值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(out, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br></pre></td></tr></table></figure>

<h3 id="可视化训练过程-1"><a href="#可视化训练过程-1" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># 画图</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着上面来</span></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        plt.cla()</span><br><span class="line">        <span class="comment"># 过了一道 softmax 的激励函数后的最大概率才是预测值</span></span><br><span class="line">        prediction = torch.<span class="built_in">max</span>(F.softmax(out), <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">        pred_y = prediction.data.numpy().squeeze()</span><br><span class="line">        target_y = y.data.numpy()</span><br><span class="line">        plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c=pred_y, s=<span class="number">100</span>, lw=<span class="number">0</span>, cmap=<span class="string">&#x27;RdYlGn&#x27;</span>)</span><br><span class="line">        accuracy = <span class="built_in">sum</span>(pred_y == target_y)/<span class="number">200.</span>  <span class="comment"># 预测中有多少和真实值一样</span></span><br><span class="line">        plt.text(<span class="number">1.5</span>, -<span class="number">4</span>, <span class="string">&#x27;Accuracy=%.2f&#x27;</span> % accuracy, fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;color&#x27;</span>:  <span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">plt.ioff()  <span class="comment"># 停止画图</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<h2 id="快速搭建网络"><a href="#快速搭建网络" class="headerlink" title="快速搭建网络"></a>快速搭建网络</h2><p>之前写神经网络时用到的步骤. 我们用 <code>net1</code> 代表这种方式搭建的神经网络.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line">        x = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net1 = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)   <span class="comment"># 这是我们用这种方式搭建的 net1</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net2 = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>对比一下两者的结构:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(net1)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (1 -&gt; 10)</span></span><br><span class="line"><span class="string">  (predict): Linear (10 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(net2)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Sequential (</span></span><br><span class="line"><span class="string">  (0): Linear (1 -&gt; 10)</span></span><br><span class="line"><span class="string">  (1): ReLU ()</span></span><br><span class="line"><span class="string">  (2): Linear (10 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>发现 <code>net2</code> 多显示了一些内容, 把激励函数也一同纳入进去了, 但是 <code>net1</code> 中, 激励函数实际上是在 <code>forward()</code> 功能中才被调用的. 这也就说明了, 相比 <code>net2</code>, <code>net1</code> 的好处就是, 你可以根据个人需要更加个性化自己的前向传播过程, 比如(RNN). 不过如果你不需要七七八八的过程,  <code>net2</code> 这种形式更适合.</p>
<h2 id="保存提取"><a href="#保存提取" class="headerlink" title="保存提取"></a>保存提取</h2><h3 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h3><p>建造数据, 搭建网络:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假数据</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)  <span class="comment"># x data (tensor), shape=(100, 1)</span></span><br><span class="line">y = x.<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.2</span>*torch.rand(x.size())  <span class="comment"># noisy y data (tensor), shape=(100, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save</span>():</span></span><br><span class="line">    <span class="comment"># 建网络</span></span><br><span class="line">    net1 = torch.nn.Sequential(</span><br><span class="line">        torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">        torch.nn.ReLU(),</span><br><span class="line">        torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    optimizer = torch.optim.SGD(net1.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">    loss_func = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        prediction = net1(x)</span><br><span class="line">        loss = loss_func(prediction, y)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>

<p>两种途径来保存</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.save(net1, <span class="string">&#x27;net.pkl&#x27;</span>)  <span class="comment"># 保存整个网络</span></span><br><span class="line">torch.save(net1.state_dict(), <span class="string">&#x27;net_params.pkl&#x27;</span>)   <span class="comment"># 只保存网络中的参数 (速度快, 占内存少)</span></span><br></pre></td></tr></table></figure>

<h3 id="提取网络"><a href="#提取网络" class="headerlink" title="提取网络"></a>提取网络</h3><p>提取整个神经网络, 网络大的时候可能会比较慢.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_net</span>():</span></span><br><span class="line">    <span class="comment"># restore entire net1 to net2</span></span><br><span class="line">    net2 = torch.load(<span class="string">&#x27;net.pkl&#x27;</span>)</span><br><span class="line">    prediction = net2(x)</span><br></pre></td></tr></table></figure>

<h3 id="只提取网络参数"><a href="#只提取网络参数" class="headerlink" title="只提取网络参数"></a>只提取网络参数</h3><p>这种方式将会提取所有的参数, 然后再放到新建网络中.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_params</span>():</span></span><br><span class="line">    <span class="comment"># 新建 net3</span></span><br><span class="line">    net3 = torch.nn.Sequential(</span><br><span class="line">        torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">        torch.nn.ReLU(),</span><br><span class="line">        torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将保存的参数复制到 net3</span></span><br><span class="line">    net3.load_state_dict(torch.load(<span class="string">&#x27;net_params.pkl&#x27;</span>))</span><br><span class="line">    prediction = net3(x)</span><br></pre></td></tr></table></figure>



<h2 id="批训练"><a href="#批训练" class="headerlink" title="批训练"></a>批训练</h2><h3 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h3><p><code>DataLoader</code> 是 torch 提供包装数据的工具. 所以你要将 (numpy array 或其他) 数据形式装换成 Tensor, 然后再放进这个包装器中. 使用 <code>DataLoader</code> 能够帮助有效地迭代数据, 举例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">5</span>      <span class="comment"># 批训练的数据个数</span></span><br><span class="line"></span><br><span class="line">x = torch.linspace(<span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>)       <span class="comment"># x data (torch tensor)</span></span><br><span class="line">y = torch.linspace(<span class="number">10</span>, <span class="number">1</span>, <span class="number">10</span>)       <span class="comment"># y data (torch tensor)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 先转换成 torch 能识别的 Dataset</span></span><br><span class="line">torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 dataset 放入 DataLoader</span></span><br><span class="line">loader = Data.DataLoader(</span><br><span class="line">    dataset=torch_dataset,      <span class="comment"># torch TensorDataset format</span></span><br><span class="line">    batch_size=BATCH_SIZE,      <span class="comment"># mini batch size</span></span><br><span class="line">    shuffle=<span class="literal">True</span>,               <span class="comment"># 要不要打乱数据 (打乱比较好)</span></span><br><span class="line">    num_workers=<span class="number">2</span>,              <span class="comment"># 多线程来读数据</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):   <span class="comment"># 训练所有!整套!数据 3 次</span></span><br><span class="line">    <span class="keyword">for</span> step, (batch_x, batch_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):  <span class="comment"># 每一步 loader 释放一小批数据用来学习</span></span><br><span class="line">        <span class="comment"># 假设这里就是你训练的地方...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打出来一些数据</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#x27;</span>, epoch, <span class="string">&#x27;| Step: &#x27;</span>, step, <span class="string">&#x27;| batch x: &#x27;</span>,</span><br><span class="line">              batch_x.numpy(), <span class="string">&#x27;| batch y: &#x27;</span>, batch_y.numpy())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Epoch:  0 | Step:  0 | batch x:  [ 6.  7.  2.  3.  1.] | batch y:  [  5.   4.   9.   8.  10.]</span></span><br><span class="line"><span class="string">Epoch:  0 | Step:  1 | batch x:  [  9.  10.   4.   8.   5.] | batch y:  [ 2.  1.  7.  3.  6.]</span></span><br><span class="line"><span class="string">Epoch:  1 | Step:  0 | batch x:  [  3.   4.   2.   9.  10.] | batch y:  [ 8.  7.  9.  2.  1.]</span></span><br><span class="line"><span class="string">Epoch:  1 | Step:  1 | batch x:  [ 1.  7.  8.  5.  6.] | batch y:  [ 10.   4.   3.   6.   5.]</span></span><br><span class="line"><span class="string">Epoch:  2 | Step:  0 | batch x:  [ 3.  9.  2.  6.  7.] | batch y:  [ 8.  2.  9.  5.  4.]</span></span><br><span class="line"><span class="string">Epoch:  2 | Step:  1 | batch x:  [ 10.   4.   8.   1.   5.] | batch y:  [  1.   7.   3.  10.   6.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>可以看出, 每步都导出了5个数据进行学习. 然后每个 epoch 的导出数据都是先打乱了以后再导出.</p>
<p>如果改变一下 <code>BATCH_SIZE = 8</code>, 这样 <code>step=0</code> 会导出8个数据, 但是, <code>step=1</code> 时数据库中的数据不够 8个</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">8</span>      <span class="comment"># 批训练的数据个数</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ...:</span><br><span class="line">    <span class="keyword">for</span> ...:</span><br><span class="line">        ...</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#x27;</span>, epoch, <span class="string">&#x27;| Step: &#x27;</span>, step, <span class="string">&#x27;| batch x: &#x27;</span>,</span><br><span class="line">              batch_x.numpy(), <span class="string">&#x27;| batch y: &#x27;</span>, batch_y.numpy())</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Epoch:  0 | Step:  0 | batch x:  [  6.   7.   2.   3.   1.   9.  10.   4.] | batch y:  [  5.   4.   9.   8.  10.   2.   1.   7.]</span></span><br><span class="line"><span class="string">Epoch:  0 | Step:  1 | batch x:  [ 8.  5.] | batch y:  [ 3.  6.]</span></span><br><span class="line"><span class="string">Epoch:  1 | Step:  0 | batch x:  [  3.   4.   2.   9.  10.   1.   7.   8.] | batch y:  [  8.   7.   9.   2.   1.  10.   4.   3.]</span></span><br><span class="line"><span class="string">Epoch:  1 | Step:  1 | batch x:  [ 5.  6.] | batch y:  [ 6.  5.]</span></span><br><span class="line"><span class="string">Epoch:  2 | Step:  0 | batch x:  [  3.   9.   2.   6.   7.  10.   4.   8.] | batch y:  [ 8.  2.  9.  5.  4.  1.  7.  3.]</span></span><br><span class="line"><span class="string">Epoch:  2 | Step:  1 | batch x:  [ 1.  5.] | batch y:  [ 10.   6.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><h3 id="MNIST手写数据"><a href="#MNIST手写数据" class="headerlink" title="MNIST手写数据"></a>MNIST手写数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torchvision      <span class="comment"># 数据库模块</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyper Parameters</span></span><br><span class="line">EPOCH = <span class="number">1</span>           <span class="comment"># 训练整批数据多少次, 为了节约时间, 我们只训练一次</span></span><br><span class="line">BATCH_SIZE = <span class="number">50</span></span><br><span class="line">LR = <span class="number">0.001</span>          <span class="comment"># 学习率</span></span><br><span class="line">DOWNLOAD_MNIST = <span class="literal">True</span>  <span class="comment"># 如果你已经下载好了mnist数据就写上 False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Mnist 手写数字</span></span><br><span class="line">train_data = torchvision.datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./mnist/&#x27;</span>,    <span class="comment"># 保存或者提取位置</span></span><br><span class="line">    train=<span class="literal">True</span>,  <span class="comment"># this is training data</span></span><br><span class="line">    transform=torchvision.transforms.ToTensor(),    <span class="comment"># 转换 PIL.Image or numpy.ndarray 成</span></span><br><span class="line">                                                    <span class="comment"># torch.FloatTensor (C x H x W), 训练的时候 normalize 成 [0.0, 1.0] 区间</span></span><br><span class="line">    download=DOWNLOAD_MNIST,          <span class="comment"># 没下载就下载, 下载了就不用再下了</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>测试数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">test_data = torchvision.datasets.MNIST(root=<span class="string">&#x27;./mnist/&#x27;</span>, train=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 批训练 50samples, 1 channel, 28x28 (50, 1, 28, 28)</span></span><br><span class="line">train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了节约时间, 我们测试时只测试前2000个</span></span><br><span class="line">test_x = torch.unsqueeze(test_data.test_data, dim=<span class="number">1</span>).<span class="built_in">type</span>(torch.FloatTensor)[:<span class="number">2000</span>]/<span class="number">255.</span>   <span class="comment"># shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)</span></span><br><span class="line">test_y = test_data.test_labels[:<span class="number">2000</span>]</span><br></pre></td></tr></table></figure>

<h3 id="CNN模型"><a href="#CNN模型" class="headerlink" title="CNN模型"></a>CNN模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(  <span class="comment"># input shape (1, 28, 28)</span></span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                in_channels=<span class="number">1</span>,      <span class="comment"># input height</span></span><br><span class="line">                out_channels=<span class="number">16</span>,    <span class="comment"># n_filters</span></span><br><span class="line">                kernel_size=<span class="number">5</span>,      <span class="comment"># filter size</span></span><br><span class="line">                stride=<span class="number">1</span>,           <span class="comment"># filter movement/step</span></span><br><span class="line">                padding=<span class="number">2</span>,      <span class="comment"># 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1</span></span><br><span class="line">            ),      <span class="comment"># output shape (16, 28, 28)</span></span><br><span class="line">            nn.ReLU(),    <span class="comment"># activation</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),    <span class="comment"># 在 2x2 空间里向下采样, output shape (16, 14, 14)</span></span><br><span class="line">        )</span><br><span class="line">        self.conv2 = nn.Sequential(  <span class="comment"># input shape (16, 14, 14)</span></span><br><span class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),  <span class="comment"># output shape (32, 14, 14)</span></span><br><span class="line">            nn.ReLU(),  <span class="comment"># activation</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),  <span class="comment"># output shape (32, 7, 7)</span></span><br><span class="line">        )</span><br><span class="line">        self.out = nn.Linear(<span class="number">32</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">10</span>)   <span class="comment"># fully connected layer, output 10 classes</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)   <span class="comment"># 展平多维的卷积图成 (batch_size, 32 * 7 * 7)</span></span><br><span class="line">        output = self.out(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">cnn = CNN()</span><br><span class="line"><span class="built_in">print</span>(cnn)  <span class="comment"># net architecture</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">CNN (</span></span><br><span class="line"><span class="string">  (conv1): Sequential (</span></span><br><span class="line"><span class="string">    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="string">    (1): ReLU ()</span></span><br><span class="line"><span class="string">    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (conv2): Sequential (</span></span><br><span class="line"><span class="string">    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="string">    (1): ReLU ()</span></span><br><span class="line"><span class="string">    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (out): Linear (1568 -&gt; 10)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   <span class="comment"># optimize all cnn parameters</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss()   <span class="comment"># the target label is not one-hotted</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># training and testing</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">    <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):   <span class="comment"># 分配 batch data, normalize x when iterate train_loader</span></span><br><span class="line">        output = cnn(b_x)               <span class="comment"># cnn output</span></span><br><span class="line">        loss = loss_func(output, b_y)   <span class="comment"># cross entropy loss</span></span><br><span class="line">        optimizer.zero_grad()           <span class="comment"># clear gradients for this training step</span></span><br><span class="line">        loss.backward()                 <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">        optimizer.step()                <span class="comment"># apply gradients</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0306 | test accuracy: 0.97</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0147 | test accuracy: 0.98</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0427 | test accuracy: 0.98</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0078 | test accuracy: 0.98</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>取10个数据, 看预测的值到底对不对:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">est_output = cnn(test_x[:<span class="number">10</span>])</span><br><span class="line">pred_y = torch.<span class="built_in">max</span>(test_output, <span class="number">1</span>)[<span class="number">1</span>].data.numpy().squeeze()</span><br><span class="line"><span class="built_in">print</span>(pred_y, <span class="string">&#x27;prediction number&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(test_y[:<span class="number">10</span>].numpy(), <span class="string">&#x27;real number&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[7 2 1 0 4 1 4 9 5 9] prediction number</span></span><br><span class="line"><span class="string">[7 2 1 0 4 1 4 9 5 9] real number</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h2 id="LSTM-分类"><a href="#LSTM-分类" class="headerlink" title="LSTM 分类"></a>LSTM 分类</h2><h3 id="MNIST手写数据-1"><a href="#MNIST手写数据-1" class="headerlink" title="MNIST手写数据"></a>MNIST手写数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> dsets</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyper Parameters</span></span><br><span class="line">EPOCH = <span class="number">1</span>           <span class="comment"># 训练整批数据多少次, 为了节约时间, 我们只训练一次</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">TIME_STEP = <span class="number">28</span>      <span class="comment"># rnn 时间步数 / 图片高度</span></span><br><span class="line">INPUT_SIZE = <span class="number">28</span>     <span class="comment"># rnn 每步输入值 / 图片每行像素</span></span><br><span class="line">LR = <span class="number">0.01</span>           <span class="comment"># learning rate</span></span><br><span class="line">DOWNLOAD_MNIST = <span class="literal">True</span>  <span class="comment"># 如果你已经下载好了mnist数据就写上 Fasle</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Mnist 手写数字</span></span><br><span class="line">train_data = torchvision.datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./mnist/&#x27;</span>,    <span class="comment"># 保存或者提取位置</span></span><br><span class="line">    train=<span class="literal">True</span>,  <span class="comment"># this is training data</span></span><br><span class="line">    transform=torchvision.transforms.ToTensor(),    <span class="comment"># 转换 PIL.Image or numpy.ndarray 成</span></span><br><span class="line">                                                    <span class="comment"># torch.FloatTensor (C x H x W), 训练的时候 normalize 成 [0.0, 1.0] 区间</span></span><br><span class="line">    download=DOWNLOAD_MNIST,          <span class="comment"># 没下载就下载, 下载了就不用再下了</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>测试数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">test_data = torchvision.datasets.MNIST(root=<span class="string">&#x27;./mnist/&#x27;</span>, train=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 批训练 50samples, 1 channel, 28x28 (50, 1, 28, 28)</span></span><br><span class="line">train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了节约时间, 我们测试时只测试前2000个</span></span><br><span class="line">test_x = torch.unsqueeze(test_data.test_data, dim=<span class="number">1</span>).<span class="built_in">type</span>(torch.FloatTensor)[:<span class="number">2000</span>]/<span class="number">255.</span>   <span class="comment"># shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)</span></span><br><span class="line">test_y = test_data.test_labels[:<span class="number">2000</span>]</span><br></pre></td></tr></table></figure>

<h3 id="RNN模型"><a href="#RNN模型" class="headerlink" title="RNN模型"></a>RNN模型</h3><p>用一个 class 来建立 RNN 模型. 这个 RNN 整体流程是</p>
<ol>
<li><code>(input0, state0)</code> -&gt; <code>LSTM</code> -&gt; <code>(output0, state1)</code>;</li>
<li><code>(input1, state1)</code> -&gt; <code>LSTM</code> -&gt; <code>(output1, state2)</code>;</li>
<li>…</li>
<li><code>(inputN, stateN)</code>-&gt; <code>LSTM</code> -&gt; <code>(outputN, stateN+1)</code>;</li>
<li><code>outputN</code> -&gt; <code>Linear</code> -&gt; <code>prediction</code>. 通过<code>LSTM</code>分析每一时刻的值, 并且将这一时刻和前面时刻的理解合并在一起, 生成当前时刻对前面数据的理解或记忆. 传递这种理解给下一时刻分析.    </li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(RNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.rnn = nn.LSTM(     <span class="comment"># LSTM 效果要比 nn.RNN() 好多了</span></span><br><span class="line">            input_size=<span class="number">28</span>,      <span class="comment"># 图片每行的数据像素点</span></span><br><span class="line">            hidden_size=<span class="number">64</span>,     <span class="comment"># rnn hidden unit</span></span><br><span class="line">            num_layers=<span class="number">1</span>,       <span class="comment"># 有几层 RNN layers</span></span><br><span class="line">            batch_first=<span class="literal">True</span>,   <span class="comment"># input &amp; output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.out = nn.Linear(<span class="number">64</span>, <span class="number">10</span>)    <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># x shape (batch, time_step, input_size)</span></span><br><span class="line">        <span class="comment"># r_out shape (batch, time_step, output_size)</span></span><br><span class="line">        <span class="comment"># h_n shape (n_layers, batch, hidden_size)   LSTM 有两个 hidden states, h_n 是分线, h_c 是主线</span></span><br><span class="line">        <span class="comment"># h_c shape (n_layers, batch, hidden_size)</span></span><br><span class="line">        r_out, (h_n, h_c) = self.rnn(x, <span class="literal">None</span>)   <span class="comment"># None 表示 hidden state 会用全0的 state</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 选取最后一个时间点的 r_out 输出</span></span><br><span class="line">        <span class="comment"># 这里 r_out[:, -1, :] 的值也是 h_n 的值</span></span><br><span class="line">        out = self.out(r_out[:, -<span class="number">1</span>, :])</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">rnn = RNN()</span><br><span class="line"><span class="built_in">print</span>(rnn)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">RNN (</span></span><br><span class="line"><span class="string">  (rnn): LSTM(28, 64, batch_first=True)</span></span><br><span class="line"><span class="string">  (out): Linear (64 -&gt; 10)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><p>将图片数据看成一个时间上的连续数据, 每一行的像素点都是这个时刻的输入, 读完整张图片就是从上而下的读完了每行的像素点. 然后我们就可以拿出 RNN 在最后一步的分析值判断图片是哪一类了. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   <span class="comment"># optimize all parameters</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss()   <span class="comment"># the target label is not one-hotted</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># training and testing</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">    <span class="keyword">for</span> step, (x, b_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):   <span class="comment"># gives batch data</span></span><br><span class="line">        b_x = x.view(-<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)   <span class="comment"># reshape x to (batch, time_step, input_size)</span></span><br><span class="line"></span><br><span class="line">        output = rnn(b_x)               <span class="comment"># rnn output</span></span><br><span class="line">        loss = loss_func(output, b_y)   <span class="comment"># cross entropy loss</span></span><br><span class="line">        optimizer.zero_grad()           <span class="comment"># clear gradients for this training step</span></span><br><span class="line">        loss.backward()                 <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">        optimizer.step()                <span class="comment"># apply gradients</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0945 | test accuracy: 0.94</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0984 | test accuracy: 0.94</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0332 | test accuracy: 0.95</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.1868 | test accuracy: 0.96</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/03/24/OpenCV/" rel="prev" title="OpenCV">
      <i class="fa fa-chevron-left"></i> OpenCV
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/08/11/oh-my-zsh-%E9%85%8D%E7%BD%AE/" rel="next" title="oh-my-zsh 配置">
      oh-my-zsh 配置 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Torch-amp-Numpy"><span class="nav-number">1.</span> <span class="nav-text">Torch &amp; Numpy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Torch-%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="nav-number">2.</span> <span class="nav-text">Torch 中的数学运算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%99%AE%E9%80%9A%E8%AE%A1%E7%AE%97"><span class="nav-number">2.1.</span> <span class="nav-text">普通计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97"><span class="nav-number">2.2.</span> <span class="nav-text">矩阵运算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Variable"><span class="nav-number">3.</span> <span class="nav-text">Variable</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Variable-%E8%AE%A1%E7%AE%97-%E6%A2%AF%E5%BA%A6"><span class="nav-number">3.1.</span> <span class="nav-text">Variable 计算, 梯度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%8E%B7%E5%8F%96-Variable-%E9%87%8C%E9%9D%A2%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="nav-number">3.2.</span> <span class="nav-text">获取 Variable 里面的数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92"><span class="nav-number">4.</span> <span class="nav-text">回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%BA%E7%AB%8B%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">4.1.</span> <span class="nav-text">建立数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%BA%E7%AB%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.2.</span> <span class="nav-text">建立神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C"><span class="nav-number">4.3.</span> <span class="nav-text">训练网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">4.4.</span> <span class="nav-text">可视化训练过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E7%B1%BB"><span class="nav-number">5.</span> <span class="nav-text">分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%BA%E7%AB%8B%E6%95%B0%E6%8D%AE%E9%9B%86-1"><span class="nav-number">5.1.</span> <span class="nav-text">建立数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%BA%E7%AB%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-1"><span class="nav-number">5.2.</span> <span class="nav-text">建立神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C-1"><span class="nav-number">5.3.</span> <span class="nav-text">训练网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B-1"><span class="nav-number">5.4.</span> <span class="nav-text">可视化训练过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E7%BD%91%E7%BB%9C"><span class="nav-number">6.</span> <span class="nav-text">快速搭建网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98%E6%8F%90%E5%8F%96"><span class="nav-number">7.</span> <span class="nav-text">保存提取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98"><span class="nav-number">7.1.</span> <span class="nav-text">保存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C"><span class="nav-number">7.2.</span> <span class="nav-text">提取网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AA%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0"><span class="nav-number">7.3.</span> <span class="nav-text">只提取网络参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%B9%E8%AE%AD%E7%BB%83"><span class="nav-number">8.</span> <span class="nav-text">批训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DataLoader"><span class="nav-number">8.1.</span> <span class="nav-text">DataLoader</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN"><span class="nav-number">9.</span> <span class="nav-text">CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MNIST%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE"><span class="nav-number">9.1.</span> <span class="nav-text">MNIST手写数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CNN%E6%A8%A1%E5%9E%8B"><span class="nav-number">9.2.</span> <span class="nav-text">CNN模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">9.3.</span> <span class="nav-text">训练</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM-%E5%88%86%E7%B1%BB"><span class="nav-number">10.</span> <span class="nav-text">LSTM 分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MNIST%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE-1"><span class="nav-number">10.1.</span> <span class="nav-text">MNIST手写数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN%E6%A8%A1%E5%9E%8B"><span class="nav-number">10.2.</span> <span class="nav-text">RNN模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="nav-number">10.3.</span> <span class="nav-text">训练</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">fanchl</p>
  <div class="site-description" itemprop="description">日拱一卒，功不唐捐。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/fanchl" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fanchl" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:fanchl@bupt.edu.cn" title="E-Mail → mailto:fanchl@bupt.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">fanchl</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
